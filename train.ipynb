{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe063b1-03f5-4a78-a417-9d664cb96fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, brier_score_loss\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# from sklearnex import patch_sklearn, config_context\n",
    "# patch_sklearn()\n",
    "\n",
    "from MLstatkit.stats import Delong_test\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略UserWarning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73660242-c2fb-4a35-8d44-16ce352e9e7c",
   "metadata": {},
   "source": [
    "test_data = pd.read_csv(\"depression_test.csv\")\n",
    "test_data['Cardiac_diseases'] = test_data[['Diabetes', 'heartdisease', 'stroke']].apply(lambda x: (x==1).sum(), axis=1)\n",
    "test_data = test_data[test_data['Cardiac_diseases']>=1]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fe21df2-284f-4a7f-ba56-a58f90999425",
   "metadata": {},
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b064402-7b0d-4507-bced-9d6d21013dd6",
   "metadata": {},
   "source": [
    "# data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cab9bf-7059-40ee-94ca-48ca009cd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data.csv\")\n",
    "data = pd.read_csv(\"depression_test_process.csv\")\n",
    "data['Cardiac_diseases'] = data[['Diabetes', 'heartdisease', 'stroke']].apply(lambda x: (x==1).sum(), axis=1)\n",
    "data = data[data['Cardiac_diseases']>=1]\n",
    "# data['Cardiac_diseases'] = data[['Diabetes', 'Heart disease', 'Stroke']].apply(lambda x: (x==1).sum(), axis=1)\n",
    "# data = data[data['Cardiac_diseases']>=1]\n",
    "# data = data.drop(columns=['depression1', 'depression2', 'depression_score', 'Diabetes', 'Heart disease', 'Stroke', 'Cardiac_diseases', 'Hypertension', 'Dyslipidemia', 'Health satisfaction', 'Medical satisfaction']+[c for c in data.columns if 'medicine' in c])\n",
    "data.replace(to_replace=['', ' '], value=np.nan, inplace=True)\n",
    "data.drop(columns=['Cardiac_diseases'], inplace=True)\n",
    "data = data.mask(data < 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb80443-d52b-4761-b74b-b2901009fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data.csv\")\n",
    "test_data = pd.read_csv(\"test_data_all_process.csv\")\n",
    "test_data['Cardiac_diseases'] = test_data[['Diabetes', 'heartdisease', 'stroke']].apply(lambda x: (x==1).sum(), axis=1)\n",
    "test_data = test_data[test_data['Cardiac_diseases']>=1]\n",
    "# data['Cardiac_diseases'] = data[['Diabetes', 'Heart disease', 'Stroke']].apply(lambda x: (x==1).sum(), axis=1)\n",
    "# data = data[data['Cardiac_diseases']>=1]\n",
    "# data = data.drop(columns=['depression1', 'depression2', 'depression_score', 'Diabetes', 'Heart disease', 'Stroke', 'Cardiac_diseases', 'Hypertension', 'Dyslipidemia', 'Health satisfaction', 'Medical satisfaction']+[c for c in data.columns if 'medicine' in c])\n",
    "test_data.replace(to_replace=['', ' '], value=np.nan, inplace=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cc1c1-4943-42cf-833e-d52ef16664ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fc8f2-0636-41e5-a246-e8988fe27efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd58a6-aee9-42a9-86f2-1a4fc8e685e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac2c37-e46e-44ed-ac88-864ba49b10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除缺失值太多的字段\n",
    "# data_ = data.dropna(axis=1, thresh=data.shape[0]*0.7)\n",
    "# test_data_ = test_data.dropna(axis=1, thresh=test_data.shape[0]*0.7)\n",
    "data_ = data.dropna(axis=0, thresh=data.shape[1]*0.7)\n",
    "test_data_ = test_data.dropna(axis=0, thresh=test_data.shape[1]*0.7)\n",
    "# data_ = data_.drop(['Unnamed: 0', 'ID'], axis=1)\n",
    "data_ = data_.drop(columns=['Diabetes', 'heartdisease', 'stroke'])\n",
    "test_data_ = test_data_.drop(columns=['Diabetes', 'heartdisease', 'stroke', 'Cardiac_diseases'])\n",
    "test_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280cd33-9316-4d89-acc4-0dd6356d9f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926a909-cd92-4e02-9b06-b5c7df9faa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "# KNN填充\n",
    "idx = data_.index\n",
    "columns = data_.columns\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=50)\n",
    "# 用 KNN 填充缺失值\n",
    "analysis_data = pd.DataFrame(knn_imputer.fit_transform(data_), columns=data_.columns)\n",
    "analysis_data.set_index(idx, inplace=True)\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f3311-086e-45f7-9e4a-f432f4eff82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "# KNN填充\n",
    "idx = test_data_.index\n",
    "columns = test_data_.columns\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=50)\n",
    "# 用 KNN 填充缺失值\n",
    "analysis_test_data = pd.DataFrame(knn_imputer.fit_transform(test_data_), columns=test_data_.columns)\n",
    "analysis_test_data.set_index(idx, inplace=True)\n",
    "analysis_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19c95e-352d-4108-a1fa-d18efb2e1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_test_data = analysis_test_data.astype(int)\n",
    "analysis_data = analysis_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb234d4c-e30a-4039-b0e1-43a547ac9010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5107705-7ffa-404b-be79-ef39daee7be0",
   "metadata": {},
   "source": [
    "# feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6e967-a356-4bc8-883b-9f358c89ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X = analysis_data.drop(columns=['depression'])\n",
    "y = analysis_data['depression']\n",
    "\n",
    "# 去掉变化小的变量\n",
    "selector = VarianceThreshold(threshold=0.05)  # 可以调整阈值\n",
    "X_var_thresh = selector.fit_transform(X)\n",
    "\n",
    "# 获取剩余特征的列名\n",
    "features_var_thresh = X.columns[selector.get_support()]\n",
    "X = analysis_data[features_var_thresh]\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# 测试集转化\n",
    "test_data_X = analysis_test_data.drop(columns=['depression'])\n",
    "test_data_y = analysis_test_data['depression']\n",
    "test_data_X = test_data_X.reset_index(drop=True)\n",
    "test_data_y = test_data_y.reset_index(drop=True)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873528d-a6e6-4b55-a730-2f44a27a9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- lasso --------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# LassoCV 进行特征选择\n",
    "lasso = LassoCV(cv=10, max_iter=30000)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# 获取非零系数的特征\n",
    "# features_lasso = X.columns[lasso.coef_>0].to_list()[:15]\n",
    "# features_lasso, len(features_lasso)\n",
    "\n",
    "# 获取所有特征的系数\n",
    "coef = lasso.coef_\n",
    "\n",
    "# 将特征名称和对应的系数放入DataFrame\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coef})\n",
    "\n",
    "# 按系数的绝对值从大到小排序\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "# 选择前15个特征\n",
    "features_lasso = coef_df.head(15)['Feature'].to_list()\n",
    "features_lasso, len(features_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87bf3c6-f6fd-4b20-8d41-e87ce475b81c",
   "metadata": {},
   "source": [
    "# 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866596d-2e35-4432-82a1-04bd6fa726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)  # precision\n",
    "    npv = tn / (tn + fn)\n",
    "    return sensitivity, specificity, ppv, npv\n",
    "\n",
    "def hosmer_lemeshow_test(y_true, y_pred_prob, g=10):\n",
    "    data = pd.DataFrame({'true': y_true, 'pred': y_pred_prob})\n",
    "    data['group'] = pd.qcut(data['pred'], g, duplicates='drop')\n",
    "    obs = data.groupby('group')['true'].sum()\n",
    "    exp = data.groupby('group')['pred'].sum()\n",
    "    n = data.groupby('group').size()\n",
    "    hl_test_stat = ((obs - exp) ** 2 / (exp * (1 - exp / n))).sum()\n",
    "    p_value = 1 - chi2.cdf(hl_test_stat, g - 2)\n",
    "    return hl_test_stat, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbc78e-2e51-4366-907b-9f4c002e71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校准曲线\n",
    "def calibration_curve_plot(y_test, y_pred_prod, file, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_pred_prod, n_bins=10)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Bias-corrected')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Ideal')\n",
    "    plt.xlabel('Predicted Pr (PCOS=1)')\n",
    "    plt.ylabel('Actual Probability')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(file, dpi=300, bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 决策曲线\n",
    "def calculate_net_benefit_model(thresh_group, y_pred_score, y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score > thresh\n",
    "        tn, fp, fn, tp = confusion_matrix(y_label, y_pred_label).ravel()\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit_model = np.append(net_benefit_model, net_benefit)\n",
    "    return net_benefit_model\n",
    "\n",
    "\n",
    "def calculate_net_benefit_all(thresh_group, y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_label, y_label).ravel()\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        net_benefit = (tp / total) - (tn / total) * (thresh / (1 - thresh))\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)\n",
    "    return net_benefit_all\n",
    "\n",
    "\n",
    "def plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all):\n",
    "    #Plot\n",
    "    ax.plot(thresh_group, net_benefit_model, color = 'crimson', label = 'Model')\n",
    "    ax.plot(thresh_group, net_benefit_all, color = 'black',label = 'Treat all')\n",
    "    ax.plot((0, 1), (0, 0), color = 'black', linestyle = ':', label = 'Treat none')\n",
    "\n",
    "    #Fill，显示出模型较于treat all和treat none好的部分\n",
    "    y2 = np.maximum(net_benefit_all, 0)\n",
    "    y1 = np.maximum(net_benefit_model, y2)\n",
    "    ax.fill_between(thresh_group, y1, y2, color = 'crimson', alpha = 0.2)\n",
    "\n",
    "    #Figure Configuration， 美化一下细节\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(net_benefit_model.min() - 0.15, net_benefit_model.max() + 0.15)#adjustify the y axis limitation\n",
    "    ax.set_xlabel(\n",
    "        xlabel = 'Threshold Probability', \n",
    "        fontdict= {'fontsize': 15}\n",
    "        )\n",
    "    ax.set_ylabel(\n",
    "        ylabel = 'Net Benefit', \n",
    "        fontdict= {'fontsize': 15}\n",
    "        )\n",
    "    ax.grid('major')\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.legend(loc = 'upper right')\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_importance(importance, file, feature_names):\n",
    "    # 对特征重要性进行排序\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    # 绘制横向柱状图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance)), importance[indices], align='center')\n",
    "    plt.yticks(range(len(importance)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()  # 反转y轴以使重要性最高的特征位于顶部\n",
    "    plt.savefig(file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc(file, roc_data):\n",
    "    plt.figure()\n",
    "    for model, roc in roc_data.items():\n",
    "        plt.plot(roc[0], roc[1], label=f'{model} (AUC = {roc[2]:.2f})')\n",
    "        \n",
    "    # 添加对角线\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    # 添加图例和标签\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve Comparison')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.savefig(file, dpi=300, bbox_inches='tight')\n",
    "    # 显示图像\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# 模型评估\n",
    "def model_eval(name, test_y, predict_y, pred_y_proba):\n",
    "    print(classification_report(test_y, predict_y))\n",
    "    print(confusion_matrix(test_y, predict_y))\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, predict_y))\n",
    "\n",
    "    auc = roc_auc_score(test_y, pred_y_proba)\n",
    "    fpr, tpr, _ = roc_curve(test_y, pred_y_proba)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    sensitivity, specificity, ppv, npv = calculate_metrics(test_y, predict_y)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"PPV:\", ppv)\n",
    "    print(\"NPV:\", npv)\n",
    "\n",
    "    # H-L test\n",
    "    hl_stat, hl_p_value = hosmer_lemeshow_test(test_y, pred_y_proba)\n",
    "    print(\"H-L test:\", hl_stat)\n",
    "    print(\"H-L test p-value:\", hl_p_value)\n",
    "\n",
    "    # brier_score_loss\n",
    "    brier_score = brier_score_loss(test_y, pred_y_proba)\n",
    "    print(\"Brier score:\", brier_score)\n",
    "\n",
    "    # 绘制校准曲线\n",
    "    cv_file = f\"output/plot/{name}_{int(auc*100)}_cv.tiff\"\n",
    "    calibration_curve_plot(test_y, pred_y_proba, cv_file)\n",
    "\n",
    "    # 绘制决策曲线\n",
    "    thresh_group = np.arange(0, 1, 0.01)\n",
    "    net_benefit_model = calculate_net_benefit_model(thresh_group, pred_y_proba, test_y)\n",
    "    net_benefit_all = calculate_net_benefit_all(thresh_group, test_y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "    dca_file = f\"output/plot/{name}_{int(auc*100)}_dca.tiff\"\n",
    "    fig.savefig(dca_file, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return auc, fpr, tpr\n",
    "\n",
    "\n",
    "class DelongTest():\n",
    "    def __init__(self,preds1,preds2,label,threshold=0.05):\n",
    "        '''\n",
    "        preds1:the output of model1\n",
    "        preds2:the output of model2\n",
    "        label :the actual label\n",
    "        '''\n",
    "        self._preds1=preds1\n",
    "        self._preds2=preds2\n",
    "        self._label=label\n",
    "        self.threshold=threshold\n",
    "        self._show_result()\n",
    "\n",
    "    def _auc(self,X, Y)->float:\n",
    "        return 1/(len(X)*len(Y)) * sum([self._kernel(x, y) for x in X for y in Y])\n",
    "\n",
    "    def _kernel(self,X, Y)->float:\n",
    "        '''\n",
    "        Mann-Whitney statistic\n",
    "        '''\n",
    "        return .5 if Y==X else int(Y < X)\n",
    "\n",
    "    def _structural_components(self,X, Y)->list:\n",
    "        V10 = [1/len(Y) * sum([self._kernel(x, y) for y in Y]) for x in X]\n",
    "        V01 = [1/len(X) * sum([self._kernel(x, y) for x in X]) for y in Y]\n",
    "        return V10, V01\n",
    "\n",
    "    def _get_S_entry(self,V_A, V_B, auc_A, auc_B)->float:\n",
    "        return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "    \n",
    "    def _z_score(self,var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "        return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB )**(.5)+ 1e-8)\n",
    "\n",
    "    def _group_preds_by_label(self,preds, actual)->list:\n",
    "        X = [p for (p, a) in zip(preds, actual) if a]\n",
    "        Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "        return X, Y\n",
    "\n",
    "    def _compute_z_p(self):\n",
    "        X_A, Y_A = self._group_preds_by_label(self._preds1, self._label)\n",
    "        X_B, Y_B = self._group_preds_by_label(self._preds2, self._label)\n",
    "\n",
    "        V_A10, V_A01 = self._structural_components(X_A, Y_A)\n",
    "        V_B10, V_B01 = self._structural_components(X_B, Y_B)\n",
    "\n",
    "        auc_A = self._auc(X_A, Y_A)\n",
    "        auc_B = self._auc(X_B, Y_B)\n",
    "\n",
    "        # Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "        var_A = (self._get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)+ self._get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "        var_B = (self._get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)+ self._get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "        covar_AB = (self._get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)+ self._get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "\n",
    "        # Two tailed test\n",
    "        z = self._z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "        p = st.norm.sf(abs(z))*2\n",
    "\n",
    "        return z,p\n",
    "\n",
    "    def _show_result(self):\n",
    "        z,p=self._compute_z_p()\n",
    "        print(f\"z score = {z:.5f};\\np value = {p:.5f};\")\n",
    "        if p < self.threshold :print(\"There is a significant difference\")\n",
    "        else:        print(\"There is NO significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3a76e-9629-4638-933e-350f150f425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(models, grid_search, data, features):\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    train_roc = {}\n",
    "    vaild_roc = {}\n",
    "    test_roc = {}\n",
    "\n",
    "    train_output = {}\n",
    "    vaild_output = {}\n",
    "    test_output = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        date = 'model'\n",
    "        plot = 'plot'\n",
    "\n",
    "        train_x, train_y, test_x, test_y, test_data_x, test_data_y = data[name]\n",
    "        \n",
    "        print(f'############################# {name} ##############################')\n",
    "        \n",
    "        # 参数搜索\n",
    "        param_grid = GridSearchCV(model, grid_search[name], cv=5)\n",
    "        param_grid.fit(train_x, train_y)\n",
    "        best_model = param_grid.best_estimator_\n",
    "        print(\"Best Parameters:\", param_grid.best_params_)\n",
    "\n",
    "        # 训练集\n",
    "        predict_train_y = best_model.predict(train_x)\n",
    "        pred_train_y_proba = best_model.predict_proba(train_x)[:, 1]\n",
    "\n",
    "        # 验证集\n",
    "        predict_y = best_model.predict(test_x)\n",
    "        pred_y_proba = best_model.predict_proba(test_x)[:, 1]\n",
    "\n",
    "        # 测试集\n",
    "        predict_test_y = best_model.predict(test_data_x)\n",
    "        pred_test_y_proba = best_model.predict_proba(test_data_x)[:, 1]\n",
    "\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"----------------------------------------------- train ------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        auc_train, fpr_train, tpr_train = model_eval(f\"{name}_train\", train_y, predict_train_y, pred_train_y_proba)\n",
    "        train_roc[name] = [fpr_train, tpr_train, auc_train]\n",
    "        train_output[name] = pred_train_y_proba\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------ eval ------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        auc, fpr_vaild, tpr_vaild = model_eval(f\"{name}_eval\", test_y, predict_y, pred_y_proba)\n",
    "        vaild_roc[name] = [fpr_vaild, tpr_vaild, auc]\n",
    "        vaild_output[name] = pred_y_proba\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"------------------------------------------------ test ------------------------------------------------\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        test_auc, fpr_test, tpr_test = model_eval(f\"{name}_test\", test_data_y, predict_test_y, pred_test_y_proba)\n",
    "        test_roc[name] = [fpr_test, tpr_test, test_auc]\n",
    "        test_output[name] = pred_test_y_proba\n",
    "\n",
    "        joblib_file = f\"output/model/{name}_{int(auc*100)}.pkl\"\n",
    "        \n",
    "        if name not in best_models:\n",
    "            best_models[name] = auc\n",
    "            joblib.dump(best_model, joblib_file)\n",
    "        elif auc >= best_models[name]:\n",
    "            joblib.dump(best_model, joblib_file)\n",
    "            best_models[name] = auc\n",
    "\n",
    "        # 特征重要性\n",
    "        importance_file = f\"output/plot/{name}_{int(auc*100)}_importance.tiff\"\n",
    "\n",
    "        if name == \"Logistic_Regression\" :\n",
    "            importances = best_model.coef_[0]\n",
    "        elif name == \"Support_Vector_Machine\" or name == \"Knn\":\n",
    "            continue\n",
    "        else:\n",
    "            importances = best_model.feature_importances_\n",
    "        \n",
    "        plot_importance(importances, importance_file, features)\n",
    "\n",
    "        print('####################################################################')\n",
    "\n",
    "    # 绘制ROC曲线\n",
    "    roc_file_train = \"output/plot/train_roc.tiff\"\n",
    "    plot_roc(roc_file_train, train_roc)\n",
    "\n",
    "    roc_file_vaild = \"output/plot/vaild_roc.tiff\"\n",
    "    plot_roc(roc_file_vaild, vaild_roc)\n",
    "    \n",
    "    roc_file_test = \"output/plot/test_roc.tiff\"\n",
    "    plot_roc(roc_file_test, test_roc)\n",
    "\n",
    "    # z score检验\n",
    "    models_list = list(models.keys())\n",
    "    print(\"################################################# train ################################################\")\n",
    "    for i in range(len(models_list) - 1):\n",
    "        for j in range(i + 1, len(models_list)):\n",
    "            z_score, p_value = Delong_test(train_y, train_output[models_list[i]], train_output[models_list[j]])\n",
    "            print(f\"Z-Score: {z_score}, P-Value: {p_value} of {models_list[i]} and {models_list[j]}\")\n",
    "\n",
    "    print(\"################################################# vaild ################################################\")\n",
    "    for i in range(len(models_list) - 1):\n",
    "        for j in range(i + 1, len(models_list)):\n",
    "            z_score, p_value = Delong_test(test_y, vaild_output[models_list[i]], vaild_output[models_list[j]])\n",
    "            print(f\"Z-Score: {z_score}, P-Value: {p_value} of {models_list[i]} and {models_list[j]}\")\n",
    "\n",
    "    print(\"################################################# test ################################################\")\n",
    "    for i in range(len(models_list) - 1):\n",
    "        for j in range(i + 1, len(models_list)):\n",
    "            z_score, p_value = Delong_test(test_data_y, test_output[models_list[i]], test_output[models_list[j]])\n",
    "            print(f\"Z-Score: {z_score}, P-Value: {p_value} of {models_list[i]} and {models_list[j]}\")\n",
    "    \n",
    "\n",
    "\n",
    "def build_and_optimize(fun, features):\n",
    "    print(f\"------------------- {fun} ----------------------\")\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.3, random_state=7909)\n",
    "    test_data_x = test_data_X[features]\n",
    "    skf = StratifiedShuffleSplit(n_splits=10)\n",
    "    for train_index, test_index in skf.split(X[features], y):\n",
    "        X_train = X.loc[train_index, features]\n",
    "        X_test = X.loc[test_index, features]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        test_data_X_scaled = scaler.fit_transform(test_data_x)\n",
    "    \n",
    "        # 使用SMOTE进行过采样\n",
    "        smote = SMOTE(random_state=269)\n",
    "        X_train_scaled_res, y_train_scaled_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "        models = {\n",
    "            \"Logistic_Regression\": LogisticRegression(class_weight='balanced'),\n",
    "            \"Support_Vector_Machine\": SVC(class_weight='balanced', probability=True),\n",
    "            \"Random_Forest\": BalancedRandomForestClassifier(),\n",
    "            \"Gradient_Boosting_Machine\": GradientBoostingClassifier(),\n",
    "            \"Xgboost\": xgboost.XGBClassifier(tree_method='gpu_hist'),\n",
    "            \"LightGBM\": lightgbm.LGBMClassifier(device='gpu',verbose=-1),\n",
    "            \"Knn\": KNeighborsClassifier()\n",
    "        }\n",
    "    \n",
    "        grids = {\n",
    "            \"Logistic_Regression\": {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']},\n",
    "            \"Support_Vector_Machine\": {'C': [0.1, 1, 10, 100], 'kernel': ['rbf', 'sigmoid'], 'gamma': ['scale', 'auto']},\n",
    "            \"Random_Forest\": {'n_estimators': [50, 100, 300, 500, 600], 'max_depth': [None, 10, 25, 40, 50], 'min_samples_split': [2, 5, 8, 12]},\n",
    "            \"Gradient_Boosting_Machine\": {'n_estimators': [50, 100, 300, 500, 600], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3, 5, 8, 12]},\n",
    "            \"Xgboost\": {'n_estimators': [50, 100, 300, 500, 600], 'max_depth': [3, 4, 6, 9], 'learning_rate': [0.01, 0.1, 0.5], 'subsample': [0.8, 1.0], 'colsample_bytree': [0.8, 1.0]},\n",
    "            \"LightGBM\": {'num_leaves': [5, 15, 31, 50, 70], 'max_depth': [-1, 15, 30], 'learning_rate': [0.01, 0.1, 0.5], 'n_estimators': [100, 300, 500], 'subsample': [0.8, 1.0], 'colsample_bytree': [0.8, 1.0]},\n",
    "            \"Knn\": {'n_neighbors': [40, 45, 50, 100, 150]}\n",
    "        }\n",
    "    \n",
    "        data = {\n",
    "            \"Logistic_Regression\": [X_train_scaled, y_train, X_test_scaled, y_test, test_data_X_scaled, test_data_y],\n",
    "            \"Support_Vector_Machine\": [X_train_scaled, y_train, X_test_scaled, y_test, test_data_X_scaled, test_data_y],\n",
    "            \"Random_Forest\": [X_train, y_train, X_test, y_test, test_data_x, test_data_y],\n",
    "            \"Gradient_Boosting_Machine\": [X_train, y_train, X_test, y_test, test_data_x, test_data_y],\n",
    "            \"Xgboost\": [X_train, y_train, X_test, y_test, test_data_x, test_data_y],\n",
    "            \"LightGBM\": [X_train, y_train, X_test, y_test, test_data_x, test_data_y],\n",
    "            \"Knn\": [X_train_scaled, y_train, X_test_scaled, y_test, test_data_X_scaled, test_data_y],\n",
    "        }\n",
    "\n",
    "        # data = [X_train_scaled, y_train, X_test_scaled, y_test, test_data_X_scaled, test_data_y]\n",
    "    \n",
    "        train_test(models, grids, data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f8f48-69ff-426c-b097-78ca634f5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"features_lasso\": features_lasso}\n",
    "for key, value in datasets.items():\n",
    "    build_and_optimize(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a32205-3887-4f1b-ae5d-884bdd8c7170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
